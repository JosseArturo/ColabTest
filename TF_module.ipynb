{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JosseArturo/ColabTest/blob/master/TF_module.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "8c0jAmG0aYSq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2YtesAfiVg4c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "80787bd7-392b-4be5-ba97-65ab3737dff0"
      },
      "cell_type": "code",
      "source": [
        "# Create TensorFlow object called tensor\n",
        "hello_constant = tf.constant('Hello World!')\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    # Run the tf.constant operation in the session\n",
        "    output = sess.run(hello_constant)\n",
        "    print(output)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Hello World!'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dF4SLN3dlNh4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "25dc6b91-cee8-4286-e082-affba6e5287b"
      },
      "cell_type": "code",
      "source": [
        "x = tf.placeholder(tf.string)\n",
        "y = tf.placeholder(tf.int32)\n",
        "z = tf.placeholder(tf.float32)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    output = sess.run(x, feed_dict={x: 'Test String', y: 123, z: 45.67})\n",
        "    print (output)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test String\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XGavBpyymwc2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e9632bbb-5587-424e-9383-847e47df3f6f"
      },
      "cell_type": "code",
      "source": [
        " \n",
        "x = tf.constant(10)\n",
        "y = tf.constant(2)\n",
        "z = tf.subtract(tf.divide(x,y),tf.cast(tf.constant(1),tf.float64))\n",
        " \n",
        "with tf.Session() as sess:\n",
        "    output = sess.run(z)\n",
        "    print(output)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XNJ6z44u9uIj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##############################\n",
        "#Logistic Classifier\n",
        "##############################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n0fW0M5Anr3Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Logistic Classifier\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def get_weights(n_features, n_labels):\n",
        "    \"\"\"\n",
        "    Return TensorFlow weights\n",
        "    :param n_features: Number of features\n",
        "    :param n_labels: Number of labels\n",
        "    :return: TensorFlow weights\n",
        "    \"\"\"\n",
        "    get_weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
        "    # TODO: Return weights\n",
        "    return get_weights\n",
        "\n",
        "\n",
        "def get_biases(n_labels):\n",
        "    \"\"\"\n",
        "    Return TensorFlow bias\n",
        "    :param n_labels: Number of labels\n",
        "    :return: TensorFlow bias\n",
        "    \"\"\"\n",
        "    get_biases = tf.Variable(tf.zeros(n_labels))\n",
        "    # TODO: Return biases\n",
        "    return get_biases\n",
        "\n",
        "\n",
        "def linear(input, w, b):\n",
        "    \"\"\"\n",
        "    Return linear function in TensorFlow\n",
        "    :param input: TensorFlow input\n",
        "    :param w: TensorFlow weights\n",
        "    :param b: TensorFlow biases\n",
        "    :return: TensorFlow linear function\n",
        "    \"\"\"\n",
        "    y=  tf.add(tf.matmul(input,w),b)\n",
        "    # TODO: Linear Function (xW + b)\n",
        "    \n",
        "    return y\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wPd2fWE76hcj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "6f07313b-406d-46f2-fcc0-d02f1db6c06b"
      },
      "cell_type": "code",
      "source": [
        "mport tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "from test import *\n",
        "\n",
        "def mnist_features_labels(n_labels):\n",
        "    \"\"\"\n",
        "    Gets the first <n> labels from the MNIST dataset\n",
        "    :param n_labels: Number of labels to use\n",
        "    :return: Tuple of feature list and label list\n",
        "    \"\"\"\n",
        "    mnist_features = []\n",
        "    mnist_labels = []\n",
        "\n",
        "    mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
        "\n",
        "    # In order to make quizzes run faster, we're only looking at 10000 images\n",
        "    for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)):\n",
        "\n",
        "        # Add features and labels if it's for the first <n>th labels\n",
        "        if mnist_label[:n_labels].any():\n",
        "            mnist_features.append(mnist_feature)\n",
        "            mnist_labels.append(mnist_label[:n_labels])\n",
        "\n",
        "    return mnist_features, mnist_labels\n",
        "\n",
        "\n",
        "# Number of features (28*28 image is 784 features)\n",
        "n_features = 784\n",
        "# Number of labels\n",
        "n_labels = 3\n",
        "\n",
        "# Features and Labels\n",
        "features = tf.placeholder(tf.float32)\n",
        "labels = tf.placeholder(tf.float32)\n",
        "\n",
        "# Weights and Biases\n",
        "w = get_weights(n_features, n_labels)\n",
        "b = get_biases(n_labels)\n",
        "\n",
        "# Linear Function xW + b\n",
        "logits = linear(features, w, b)\n",
        "\n",
        "# Training data\n",
        "train_features, train_labels = mnist_features_labels(n_labels)\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "with tf.Session() as session:\n",
        "    # TODO: Initialize session variables\n",
        "    session.run(init)\n",
        "    # Softmax\n",
        "    prediction = tf.nn.softmax(logits)\n",
        "\n",
        "    # Cross entropy\n",
        "    # This quantifies how far off the predictions were.\n",
        "    # You'll learn more about this in future lessons.\n",
        "    cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)\n",
        "\n",
        "    # Training loss\n",
        "    # You'll learn more about this in future lessons.\n",
        "    loss = tf.reduce_mean(cross_entropy)\n",
        "\n",
        "    # Rate at which the weights are changed\n",
        "    # You'll learn more about this in future lessons.\n",
        "    learning_rate = 0.08\n",
        "\n",
        "    # Gradient Descent\n",
        "    # This is the method used to train the model\n",
        "    # You'll learn more about this in future lessons.\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
        "\n",
        "    # Run optimizer and get loss\n",
        "    _, l = session.run(\n",
        "        [optimizer, loss],\n",
        "        feed_dict={features: train_features, labels: train_labels})\n",
        "\n",
        "# Print loss\n",
        "print('Loss: {}'.format(l))\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /datasets/ud730/mnist/train-images-idx3-ubyte.gz\n",
            "Extracting /datasets/ud730/mnist/train-labels-idx1-ubyte.gz\n",
            "Extracting /datasets/ud730/mnist/t10k-images-idx3-ubyte.gz\n",
            "Extracting /datasets/ud730/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Loss: 15.78938102722168\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NJqB7_yf9ZsI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7b9fc2fd-3ac0-48e2-c8ff-503b874227d3"
      },
      "cell_type": "code",
      "source": [
        "##Grader done by Udacity\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.framework.errors import FailedPreconditionError\n",
        "import os\n",
        "import sys\n",
        "\n",
        "def is_weights_good(w):\n",
        "    w_answer = [[-0.01811021,  0.51838213],\n",
        " [ 0.05832403, -0.48847285],\n",
        " [-0.37598562, -0.7711397 ],\n",
        " [-0.5922465, -0.3118519 ],\n",
        " [ 0.21055079, -1.1010232 ]] \n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        w_result = sess.run(w)\n",
        "      \n",
        "    return np.allclose(w_answer, w_result)\n",
        "\n",
        "\n",
        "def is_biases_good(b):\n",
        "    b_answer = [0.0, 0.0]\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        b_result = sess.run(b)\n",
        "        \n",
        "    return np.array_equal(b_answer, b_result)\n",
        "\n",
        "\n",
        "def is_linear_good(l, test_input):\n",
        "    \n",
        "    logits_answer = [[-2.34565091, -9.52450562],[-8.03849602, -9.28480148]]\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        logits_result = sess.run(l, feed_dict={test_input: [[1,2,3,4,5], [6,7,8,9,0]]})\n",
        "        \n",
        "    return np.allclose(logits_answer, logits_result)\n",
        "\n",
        "def get_result(get_weights, get_biases, linear):\n",
        "    result = {\n",
        "        'correct': False,\n",
        "        'feedback': 'That\\'s the wrong answer.',\n",
        "        'comment': ''}\n",
        "\n",
        "    tf.set_random_seed(123456)\n",
        "    \n",
        "    n_features = 5\n",
        "    n_labels = 2\n",
        "    test_input = tf.placeholder(tf.float32)\n",
        "    \n",
        "    weights = get_weights(n_features, n_labels)\n",
        "    biases = get_biases(n_labels)\n",
        "    lin = linear(test_input, weights, biases)\n",
        "\n",
        "    if not isinstance(weights, tf.Variable):\n",
        "        result['feedback'] = 'Function weights not returning tf.Variable type.'\n",
        "        result['comment'] = 'Use the tf.Variable function.'\n",
        "    elif not isinstance(biases, tf.Variable):\n",
        "        result['feedback'] = 'Function biases not returning tf.Variable type.'\n",
        "        result['comment'] = 'Use the tf.Variable function.'\n",
        "    elif weights.get_shape() != (n_features, n_labels):\n",
        "        result['feedback'] = 'Function weights is returning the wrong shape.'\n",
        "    elif biases.get_shape() != n_labels:\n",
        "        result['feedback'] = 'Function biases is returning the wrong shape.'\n",
        "    elif not is_weights_good(weights):\n",
        "        result['feedback'] = 'Function weights isn\\'t correct.'\n",
        "    elif not is_biases_good(biases):\n",
        "        result['feedback'] = 'Function biases isn\\'t correct.'\n",
        "    elif not is_linear_good(lin, test_input):\n",
        "        import pdb;pdb.set_trace()\n",
        "        result['feedback'] = 'Function linear isn\\'t correct.'\n",
        "    else:\n",
        "        try:\n",
        "            std_out = sys.stdout\n",
        "            f = open(os.devnull, 'w')\n",
        "            sys.stdout = f\n",
        "        \n",
        "        except FailedPreconditionError as err:\n",
        "            if err.message.startswith('Attempting to use uninitialized value Variable'):\n",
        "                result['feedback'] = 'At least one variable is not initialized.'\n",
        "            else:\n",
        "                raise err\n",
        "        else:\n",
        "            result['correct'] = True\n",
        "            result['feedback'] = 'You got it!  That\\'s the correct answer.'\n",
        "        finally:\n",
        "            sys.stdout = std_out\n",
        "    return result\n",
        "\n",
        "def run_grader(get_weights, get_biases, linear):\n",
        "    \n",
        "    try:\n",
        "    # Get grade result information\n",
        "        result = get_result(get_weights, get_biases, linear)\n",
        "    except Exception as err:\n",
        "        # Default error result\n",
        "        result = {\n",
        "            'correct': False,\n",
        "            'feedback': 'Something went wrong with your submission:',\n",
        "            'comment': str(err)}\n",
        "\n",
        "    feedback = result.get('feedback')\n",
        "    comment = result.get('comment')\n",
        "\n",
        "    print(f\"{feedback}\\n{comment}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_grader(get_weights, get_biases, linear)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Function weights isn't correct.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BaCqJvDt6n8A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0a0244ab-a8a9-4bb8-c2a9-555ae7a723e8"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "### DON'T MODIFY ANYTHING BELOW ###\n",
        "### Be sure to run all cells above before running this cell ###\n",
        "#import grader\n",
        "\n",
        "try:\n",
        "    run_grader(get_weights, get_biases, linear)\n",
        "except Exception as err:\n",
        "    print(str(err))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Function weights isn't correct.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YM4M2_t29ypJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##############################\n",
        "#Logistic Classifier ends\n",
        "##############################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "klc7atrO9Y65",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##############################\n",
        "#Softmax function\n",
        "##############################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3BUScLk6q4x4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b27d244a-39fc-4177-aaab-afcb10d2282f"
      },
      "cell_type": "code",
      "source": [
        "# Solution is available in the other \"solution.py\" tab\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    # TODO: Compute and return softmax(x)\n",
        "    return np.exp(x)/np.sum(np.exp(x))\n",
        "    \n",
        "logits = [3.0, 1.0, 0.2]\n",
        "print(softmax(logits))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.8360188  0.11314284 0.05083836]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nJEKzT-JtfkS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##############################\n",
        "#Softmax function TENSOR\n",
        "##############################\n",
        "\n",
        "[0.6590012 0.242433  0.0985659]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "02n4xpqVuLSy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "95737fc8-b97a-420f-d22d-659db0e877e0"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def run():\n",
        "    output = None\n",
        "    logit_data = [2.0, 1.0, 0.1]\n",
        "    logits = tf.constant(logit_data)\n",
        "    #logits = tf.placeholder(tf.float32)\n",
        "    \n",
        "    # TODO: Calculate the softmax of the logits\n",
        "    softmax = tf.nn.softmax(logits)\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "        pass\n",
        "        # TODO: Feed in the logit data\n",
        "        output = sess.run(softmax, feed_dict={logits:logit_data})   \n",
        "   \n",
        "    return output\n",
        "  \n",
        "out = run()\n",
        "print (out)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.6590012 0.242433  0.0985659]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Qs2uYu2i4SEz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##############################\n",
        "#Mini Batch\n",
        "##############################\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aN4sOyws-qG5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "def batches(batch_size, features, labels):\n",
        "    \"\"\"\n",
        "    Create batches of features and labels\n",
        "    :param batch_size: The batch size\n",
        "    :param features: List of features\n",
        "    :param labels: List of labels\n",
        "    :return: Batches of (Features, Labels)\n",
        "    \"\"\"\n",
        "    assert len(features) == len(labels)\n",
        "    outout_batches = []\n",
        "    \n",
        "    sample_size = len(features)\n",
        "    for start_i in range(0, sample_size, batch_size):\n",
        "        end_i = start_i + batch_size\n",
        "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
        "        outout_batches.append(batch)\n",
        "        \n",
        "    return outout_batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BGBdB3ln4Sxz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c64d1c6a-4509-4a8c-95d8-a6f1295f104b"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "learning_rate = 0.05\n",
        "n_input = 784  # MNIST data input (img shape: 28*28)\n",
        "n_classes = 10  # MNIST total classes (0-9 digits)\n",
        "\n",
        "# Import MNIST data\n",
        "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
        "\n",
        "# The features are already scaled and the data is shuffled\n",
        "train_features = mnist.train.images\n",
        "test_features = mnist.test.images\n",
        "\n",
        "train_labels = mnist.train.labels.astype(np.float32)\n",
        "test_labels = mnist.test.labels.astype(np.float32)\n",
        "\n",
        "# Features and Labels\n",
        "features = tf.placeholder(tf.float32, [None, n_input])\n",
        "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
        "\n",
        "# Weights & bias\n",
        "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
        "bias = tf.Variable(tf.random_normal([n_classes]))\n",
        "\n",
        "# Logits - xW + b\n",
        "logits = tf.add(tf.matmul(features, weights), bias)\n",
        "\n",
        "# Define loss and optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# Calculate accuracy\n",
        "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /datasets/ud730/mnist/train-images-idx3-ubyte.gz\n",
            "Extracting /datasets/ud730/mnist/train-labels-idx1-ubyte.gz\n",
            "Extracting /datasets/ud730/mnist/t10k-images-idx3-ubyte.gz\n",
            "Extracting /datasets/ud730/mnist/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DgCLrU_Y-d6Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0a1e5bbd-819b-4694-f382-412b283b1458"
      },
      "cell_type": "code",
      "source": [
        "# TODO: Set batch size\n",
        "batch_size = 32\n",
        "assert batch_size is not None, 'You must set the batch size'\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for i in range(100): #Added to get more iteractions an improve gradient retropropagation\n",
        "        # TODO: Train optimizer on all batches\n",
        "        for batch_features, batch_labels in batches(batch_size, train_features, train_labels):\n",
        "        # for batch_features, batch_labels in ______\n",
        "            sess.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})\n",
        "\n",
        "    # Calculate accuracy for test dataset\n",
        "    test_accuracy = sess.run(accuracy, feed_dict={features: test_features, labels: test_labels})\n",
        "\n",
        "print('Test Accuracy: {}'.format(test_accuracy))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.9190999865531921\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-cRQ4RU5_kPQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##############################\n",
        "#Whole Process with Epochs\n",
        "##############################\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZMchBMo9Ji2R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from helper import batches  # Helper function created in Mini-batching section\n",
        "\n",
        "\n",
        "def print_epoch_stats(epoch_i, sess, last_features, last_labels):\n",
        "    \"\"\"\n",
        "    Print cost and validation accuracy of an epoch\n",
        "    \"\"\"\n",
        "    current_cost = sess.run(\n",
        "        cost,\n",
        "        feed_dict={features: last_features, labels: last_labels})\n",
        "    valid_accuracy = sess.run(\n",
        "        accuracy,\n",
        "        feed_dict={features: valid_features, labels: valid_labels})\n",
        "    print('Epoch: {:<4} - Cost: {:<8.3} Valid Accuracy: {:<5.3}'.format(\n",
        "        epoch_i,\n",
        "        current_cost,\n",
        "        valid_accuracy))\n",
        "\n",
        "n_input = 784  # MNIST data input (img shape: 28*28)\n",
        "n_classes = 10  # MNIST total classes (0-9 digits)\n",
        "\n",
        "# Import MNIST data\n",
        "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
        "\n",
        "# The features are already scaled and the data is shuffled\n",
        "train_features = mnist.train.images\n",
        "valid_features = mnist.validation.images\n",
        "test_features = mnist.test.images\n",
        "\n",
        "train_labels = mnist.train.labels.astype(np.float32)\n",
        "valid_labels = mnist.validation.labels.astype(np.float32)\n",
        "test_labels = mnist.test.labels.astype(np.float32)\n",
        "\n",
        "# Features and Labels\n",
        "features = tf.placeholder(tf.float32, [None, n_input])\n",
        "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
        "\n",
        "# Weights & bias\n",
        "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
        "bias = tf.Variable(tf.random_normal([n_classes]))\n",
        "\n",
        "# Logits - xW + b\n",
        "logits = tf.add(tf.matmul(features, weights), bias)\n",
        "\n",
        "# Define loss and optimizer\n",
        "learning_rate = tf.placeholder(tf.float32)\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# Calculate accuracy\n",
        "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "learn_rate = 0.001\n",
        "\n",
        "train_batches = batches(batch_size, train_features, train_labels)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "\n",
        "    # Training cycle\n",
        "    for epoch_i in range(epochs):\n",
        "\n",
        "        # Loop over all batches\n",
        "        for batch_features, batch_labels in train_batches:\n",
        "            train_feed_dict = {\n",
        "                features: batch_features,\n",
        "                labels: batch_labels,\n",
        "                learning_rate: learn_rate}\n",
        "            sess.run(optimizer, feed_dict=train_feed_dict)\n",
        "\n",
        "        # Print cost and validation accuracy of an epoch\n",
        "        print_epoch_stats(epoch_i, sess, batch_features, batch_labels)\n",
        "\n",
        "    # Calculate accuracy for test dataset\n",
        "    test_accuracy = sess.run(\n",
        "        accuracy,\n",
        "        feed_dict={features: test_features, labels: test_labels})\n",
        "\n",
        "print('Test Accuracy: {}'.format(test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V43_IFEiLyvp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##############################\n",
        "#Complete Lab\n",
        "##############################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9vICC5B2LzBx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "50b0f9aa-031a-473f-d99a-229a95459fa7"
      },
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "import os\n",
        "import pickle\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.utils import resample\n",
        "from tqdm import tqdm\n",
        "from zipfile import ZipFile\n",
        "\n",
        "print('All modules imported.')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All modules imported.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ab4gtAEbL3FG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "17ed01b7-a3b9-4414-b4c8-8539c0c8a6f4"
      },
      "cell_type": "code",
      "source": [
        "def download(url, file):\n",
        "    \"\"\"\n",
        "    Download file from <url>\n",
        "    :param url: URL to file\n",
        "    :param file: Local file path\n",
        "    \"\"\"\n",
        "    if not os.path.isfile(file):\n",
        "        print('Downloading ' + file + '...')\n",
        "        urlretrieve(url, file)\n",
        "        print('Download Finished')\n",
        "\n",
        "##Download Parto from AMAZON\n",
        "#PUT HERE THE CODE\n",
        "\n",
        "# Wait until you see that all files have been downloaded.\n",
        "print('All files downloaded.')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All files downloaded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tbgH6WFBL-Ga",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "bb0f0400-5c86-4564-f7e8-f9a03cd5c5b8"
      },
      "cell_type": "code",
      "source": [
        "def uncompress_features_labels(file):\n",
        "    \"\"\"\n",
        "    Uncompress features and labels from a zip file\n",
        "    :param file: The zip file to extract the data from\n",
        "    \"\"\"\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    with ZipFile(file) as zipf:\n",
        "        # Progress Bar\n",
        "        filenames_pbar = tqdm(zipf.namelist(), unit='files')\n",
        "        \n",
        "        # Get features and labels from all files\n",
        "        for filename in filenames_pbar:\n",
        "            # Check if the file is a directory\n",
        "            if not filename.endswith('/'):\n",
        "                with zipf.open(filename) as image_file:\n",
        "                    image = Image.open(image_file)\n",
        "                    image.load()\n",
        "                    # Load image data as 1 dimensional array\n",
        "                    # We're using float32 to save on memory space\n",
        "                    feature = np.array(image, dtype=np.float32).flatten()\n",
        "\n",
        "                # Get the the letter from the filename.  This is the letter of the image.\n",
        "                label = os.path.split(filename)[1][0]\n",
        "\n",
        "                features.append(feature)\n",
        "                labels.append(label)\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# Get the features and labels from the zip files\n",
        "train_features, train_labels = uncompress_features_labels('notMNIST_train.zip')\n",
        "test_features, test_labels = uncompress_features_labels('notMNIST_test.zip')\n",
        "\n",
        "# Limit the amount of data to work with a docker container\n",
        "docker_size_limit = 150000\n",
        "train_features, train_labels = resample(train_features, train_labels, n_samples=docker_size_limit)\n",
        "\n",
        "# Set flags for feature engineering.  This will prevent you from skipping an important step.\n",
        "is_features_normal = False\n",
        "is_labels_encod = False\n",
        "\n",
        "# Wait until you see that all features and labels have been uncompressed.\n",
        "print('All features and labels uncompressed.')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 210001/210001 [00:46<00:00, 4532.97files/s]\n",
            "100%|██████████| 10001/10001 [00:02<00:00, 4561.28files/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "All features and labels uncompressed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NLQ8oRzRMA-Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ea315f65-19f1-4594-b96d-929968bf9ada"
      },
      "cell_type": "code",
      "source": [
        "# Problem 1 - Implement Min-Max scaling for grayscale image data\n",
        "def normalize_grayscale(image_data):\n",
        "    \"\"\"\n",
        "    Normalize the image data with Min-Max scaling to a range of [0.1, 0.9]\n",
        "    :param image_data: The image data to be normalized\n",
        "    :return: Normalized image data\n",
        "    \"\"\"\n",
        "    # TODO: Implement Min-Max scaling for grayscale image data\n",
        "    a = 0.1\n",
        "    b = 0.9\n",
        "    grayscale_min = 0\n",
        "    grayscale_max = 255\n",
        "    return a + ( ( (image_data - grayscale_min)*(b - a) )/( grayscale_max - grayscale_min)) \n",
        "\n",
        "### DON'T MODIFY ANYTHING BELOW ###\n",
        "# Test Cases\n",
        "np.testing.assert_array_almost_equal(\n",
        "    normalize_grayscale(np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 255])),\n",
        "    [0.1, 0.103137254902, 0.106274509804, 0.109411764706, 0.112549019608, 0.11568627451, 0.118823529412, 0.121960784314,\n",
        "     0.125098039216, 0.128235294118, 0.13137254902, 0.9],\n",
        "    decimal=3)\n",
        "np.testing.assert_array_almost_equal(\n",
        "    normalize_grayscale(np.array([0, 1, 10, 20, 30, 40, 233, 244, 254,255])),\n",
        "    [0.1, 0.103137254902, 0.13137254902, 0.162745098039, 0.194117647059, 0.225490196078, 0.830980392157, 0.865490196078,\n",
        "     0.896862745098, 0.9])\n",
        "\n",
        "if not is_features_normal:\n",
        "    train_features = normalize_grayscale(train_features)\n",
        "    test_features = normalize_grayscale(test_features)\n",
        "    is_features_normal = True\n",
        "\n",
        "print('Tests Passed!')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests Passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oGet7YgZMCMT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b6aac6e0-bbdf-47d9-eb7d-c7b8058d496a"
      },
      "cell_type": "code",
      "source": [
        "if not is_labels_encod:\n",
        "    # Turn labels into numbers and apply One-Hot Encoding\n",
        "    encoder = LabelBinarizer()\n",
        "    encoder.fit(train_labels)\n",
        "    train_labels = encoder.transform(train_labels)\n",
        "    test_labels = encoder.transform(test_labels)\n",
        "\n",
        "    # Change to float32, so it can be multiplied against the features in TensorFlow, which are float32\n",
        "    train_labels = train_labels.astype(np.float32)\n",
        "    test_labels = test_labels.astype(np.float32)\n",
        "    is_labels_encod = True\n",
        "\n",
        "print('Labels One-Hot Encoded')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Labels One-Hot Encoded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Lgu_Ey53MEL5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fa070289-e623-445b-a0b5-07a07e6aa50b"
      },
      "cell_type": "code",
      "source": [
        "assert is_features_normal, 'You skipped the step to normalize the features'\n",
        "assert is_labels_encod, 'You skipped the step to One-Hot Encode the labels'\n",
        "\n",
        "# Get randomized datasets for training and validation\n",
        "train_features, valid_features, train_labels, valid_labels = train_test_split(\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    test_size=0.05,\n",
        "    random_state=832289)\n",
        "\n",
        "print('Training features and labels randomized and split.')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training features and labels randomized and split.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5dHtQFOwMHkd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "553f363c-6267-48b3-cd14-868f29e714b2"
      },
      "cell_type": "code",
      "source": [
        "# Save the data for easy access\n",
        "pickle_file = 'notMNIST.pickle'\n",
        "if not os.path.isfile(pickle_file):\n",
        "    print('Saving data to pickle file...')\n",
        "    try:\n",
        "        with open('notMNIST.pickle', 'wb') as pfile:\n",
        "            pickle.dump(\n",
        "                {\n",
        "                    'train_dataset': train_features,\n",
        "                    'train_labels': train_labels,\n",
        "                    'valid_dataset': valid_features,\n",
        "                    'valid_labels': valid_labels,\n",
        "                    'test_dataset': test_features,\n",
        "                    'test_labels': test_labels,\n",
        "                },\n",
        "                pfile, pickle.HIGHEST_PROTOCOL)\n",
        "    except Exception as e:\n",
        "        print('Unable to save data to', pickle_file, ':', e)\n",
        "        raise\n",
        "\n",
        "print('Data cached in pickle file.')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving data to pickle file...\n",
            "Data cached in pickle file.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5nMu57-pMJsj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d2d3e43e-f4e3-4e95-a21a-7d08e9d958e2"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# Load the modules\n",
        "import pickle\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reload the data\n",
        "pickle_file = 'notMNIST.pickle'\n",
        "with open(pickle_file, 'rb') as f:\n",
        "  pickle_data = pickle.load(f)\n",
        "  train_features = pickle_data['train_dataset']\n",
        "  train_labels = pickle_data['train_labels']\n",
        "  valid_features = pickle_data['valid_dataset']\n",
        "  valid_labels = pickle_data['valid_labels']\n",
        "  test_features = pickle_data['test_dataset']\n",
        "  test_labels = pickle_data['test_labels']\n",
        "  del pickle_data  # Free up memory\n",
        "\n",
        "\n",
        "print('Data and modules loaded.')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data and modules loaded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f1gk-HUdMNWA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "d1f444e6-df3f-47e0-a9ae-1221f466872f"
      },
      "cell_type": "code",
      "source": [
        "features_count = 784\n",
        "labels_count = 10\n",
        "\n",
        "# TODO: Set the features and labels tensors\n",
        "features = tf.placeholder(tf.float32)\n",
        "labels = tf.placeholder(tf.float32)\n",
        "\n",
        "# TODO: Set the weights and biases tensors\n",
        "weights = tf.Variable(tf.truncated_normal((features_count, labels_count)))\n",
        "biases = tf.Variable(tf.zeros(labels_count))\n",
        "\n",
        "\n",
        "\n",
        "### DON'T MODIFY ANYTHING BELOW ###\n",
        "\n",
        "#Test Cases\n",
        "from tensorflow.python.ops.variables import Variable\n",
        "\n",
        "assert features._op.name.startswith('Placeholder'), 'features must be a placeholder'\n",
        "assert labels._op.name.startswith('Placeholder'), 'labels must be a placeholder'\n",
        "assert isinstance(weights, Variable), 'weights must be a TensorFlow variable'\n",
        "assert isinstance(biases, Variable), 'biases must be a TensorFlow variable'\n",
        "\n",
        "assert features._shape == None or (\\\n",
        "    features._shape.dims[0].value is None and\\\n",
        "    features._shape.dims[1].value in [None, 784]), 'The shape of features is incorrect'\n",
        "assert labels._shape  == None or (\\\n",
        "    labels._shape.dims[0].value is None and\\\n",
        "    labels._shape.dims[1].value in [None, 10]), 'The shape of labels is incorrect'\n",
        "assert weights._variable._shape == (784, 10), 'The shape of weights is incorrect'\n",
        "assert biases._variable._shape == (10), 'The shape of biases is incorrect'\n",
        "\n",
        "assert features._dtype == tf.float32, 'features must be type float32'\n",
        "assert labels._dtype == tf.float32, 'labels must be type float32'\n",
        "\n",
        "# Feed dicts for training, validation, and test session\n",
        "train_feed_dict = {features: train_features, labels: train_labels}\n",
        "valid_feed_dict = {features: valid_features, labels: valid_labels}\n",
        "test_feed_dict = {features: test_features, labels: test_labels}\n",
        "\n",
        "# Linear Function WX + b\n",
        "logits = tf.matmul(features, weights) + biases\n",
        "\n",
        "prediction = tf.nn.softmax(logits)\n",
        "\n",
        "# Cross entropy\n",
        "cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), axis=1)\n",
        "\n",
        "# some students have encountered challenges using this function, and have resolved issues\n",
        "# using https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits\n",
        "# please see this thread for more detail https://discussions.udacity.com/t/accuracy-0-10-in-the-intro-to-tensorflow-lab/272469/9\n",
        "\n",
        "# Training loss\n",
        "loss = tf.reduce_mean(cross_entropy)\n",
        "\n",
        "# Create an operation that initializes all variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Test Cases\n",
        "with tf.Session() as session:\n",
        "    session.run(init)\n",
        "    session.run(loss, feed_dict=train_feed_dict)\n",
        "    session.run(loss, feed_dict=valid_feed_dict)\n",
        "    session.run(loss, feed_dict=test_feed_dict)\n",
        "    biases_data = session.run(biases)\n",
        "\n",
        "assert not np.count_nonzero(biases_data), 'biases must be zeros'\n",
        "\n",
        "print('Tests Passed!')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
            "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
            "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
            "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
            "Tests Passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1TZlxOlBMVw-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ed5e7d9c-8f73-4c34-a890-42c89dac5e62"
      },
      "cell_type": "code",
      "source": [
        "# Determine if the predictions are correct\n",
        "is_correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))\n",
        "# Calculate the accuracy of the predictions\n",
        "accuracy = tf.reduce_mean(tf.cast(is_correct_prediction, tf.float32))\n",
        "\n",
        "print('Accuracy function created.')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy function created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5jZyHCheMXE8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "outputId": "62e2415c-ee32-4258-9efa-95dcb740166e"
      },
      "cell_type": "code",
      "source": [
        "# TODO: Find the best parameters for each configuration\n",
        "epochs = 10\n",
        "batch_size = 128\n",
        "learning_rate = 0.28\n",
        "\n",
        "\n",
        "\n",
        "### DON'T MODIFY ANYTHING BELOW ###\n",
        "# Gradient Descent\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)    \n",
        "\n",
        "# The accuracy measured against the validation set\n",
        "validation_accuracy = 0.0\n",
        "\n",
        "# Measurements use for graphing loss and accuracy\n",
        "log_batch_step = 50\n",
        "batches = []\n",
        "loss_batch = []\n",
        "train_acc_batch = []\n",
        "valid_acc_batch = []\n",
        "\n",
        "with tf.Session() as session:\n",
        "    session.run(init)\n",
        "    batch_count = int(math.ceil(len(train_features)/batch_size))\n",
        "\n",
        "    for epoch_i in range(epochs):\n",
        "        \n",
        "        # Progress bar\n",
        "        batches_pbar = tqdm(range(batch_count), desc='Epoch {:>2}/{}'.format(epoch_i+1, epochs), unit='batches')\n",
        "        \n",
        "        # The training cycle\n",
        "        for batch_i in batches_pbar:\n",
        "            # Get a batch of training features and labels\n",
        "            batch_start = batch_i*batch_size\n",
        "            batch_features = train_features[batch_start:batch_start + batch_size]\n",
        "            batch_labels = train_labels[batch_start:batch_start + batch_size]\n",
        "\n",
        "            # Run optimizer and get loss\n",
        "            _, l = session.run(\n",
        "                [optimizer, loss],\n",
        "                feed_dict={features: batch_features, labels: batch_labels})\n",
        "\n",
        "            # Log every 50 batches\n",
        "            if not batch_i % log_batch_step:\n",
        "                # Calculate Training and Validation accuracy\n",
        "                training_accuracy = session.run(accuracy, feed_dict=train_feed_dict)\n",
        "                validation_accuracy = session.run(accuracy, feed_dict=valid_feed_dict)\n",
        "\n",
        "                # Log batches\n",
        "                previous_batch = batches[-1] if batches else 0\n",
        "                batches.append(log_batch_step + previous_batch)\n",
        "                loss_batch.append(l)\n",
        "                train_acc_batch.append(training_accuracy)\n",
        "                valid_acc_batch.append(validation_accuracy)\n",
        "\n",
        "        # Check accuracy against Validation data\n",
        "        validation_accuracy = session.run(accuracy, feed_dict=valid_feed_dict)\n",
        "\n",
        "loss_plot = plt.subplot(211)\n",
        "loss_plot.set_title('Loss')\n",
        "loss_plot.plot(batches, loss_batch, 'g')\n",
        "loss_plot.set_xlim([batches[0], batches[-1]])\n",
        "acc_plot = plt.subplot(212)\n",
        "acc_plot.set_title('Accuracy')\n",
        "acc_plot.plot(batches, train_acc_batch, 'r', label='Training Accuracy')\n",
        "acc_plot.plot(batches, valid_acc_batch, 'x', label='Validation Accuracy')\n",
        "acc_plot.set_ylim([0, 1.0])\n",
        "acc_plot.set_xlim([batches[0], batches[-1]])\n",
        "acc_plot.legend(loc=4)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('Validation accuracy at {}'.format(validation_accuracy))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  1/10: 100%|██████████| 1114/1114 [00:08<00:00, 138.37batches/s]\n",
            "Epoch  2/10: 100%|██████████| 1114/1114 [00:08<00:00, 132.85batches/s]\n",
            "Epoch  3/10: 100%|██████████| 1114/1114 [00:08<00:00, 132.98batches/s]\n",
            "Epoch  4/10: 100%|██████████| 1114/1114 [00:08<00:00, 134.99batches/s]\n",
            "Epoch  5/10: 100%|██████████| 1114/1114 [00:08<00:00, 133.02batches/s]\n",
            "Epoch  6/10: 100%|██████████| 1114/1114 [00:08<00:00, 133.25batches/s]\n",
            "Epoch  7/10: 100%|██████████| 1114/1114 [00:08<00:00, 132.48batches/s]\n",
            "Epoch  8/10: 100%|██████████| 1114/1114 [00:08<00:00, 133.42batches/s]\n",
            "Epoch  9/10: 100%|██████████| 1114/1114 [00:08<00:00, 132.54batches/s]\n",
            "Epoch 10/10: 100%|██████████| 1114/1114 [00:08<00:00, 132.78batches/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSim3pAlKKaWUW/p/DPPDuZiMBpEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Validation accuracy at 0.7282666563987732\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RE_moMPuMctW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "5585b103-04fe-47ae-d7be-815140481e52"
      },
      "cell_type": "code",
      "source": [
        "# TODO: Set the epochs, batch_size, and learning_rate with the best parameters from problem 3\n",
        "epochs = 4\n",
        "batch_size = 100\n",
        "learning_rate = 0.2\n",
        "\n",
        "\n",
        "\n",
        "### DON'T MODIFY ANYTHING BELOW ###\n",
        "# The accuracy measured against the test set\n",
        "test_accuracy = 0.0\n",
        "\n",
        "with tf.Session() as session:\n",
        "    \n",
        "    session.run(init)\n",
        "    batch_count = int(math.ceil(len(train_features)/batch_size))\n",
        "\n",
        "    for epoch_i in range(epochs):\n",
        "        \n",
        "        # Progress bar\n",
        "        batches_pbar = tqdm(range(batch_count), desc='Epoch {:>2}/{}'.format(epoch_i+1, epochs), unit='batches')\n",
        "        \n",
        "        # The training cycle\n",
        "        for batch_i in batches_pbar:\n",
        "            # Get a batch of training features and labels\n",
        "            batch_start = batch_i*batch_size\n",
        "            batch_features = train_features[batch_start:batch_start + batch_size]\n",
        "            batch_labels = train_labels[batch_start:batch_start + batch_size]\n",
        "\n",
        "            # Run optimizer\n",
        "            _ = session.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})\n",
        "\n",
        "        # Check accuracy against Test data\n",
        "        test_accuracy = session.run(accuracy, feed_dict=test_feed_dict)\n",
        "\n",
        "\n",
        "assert test_accuracy >= 0.80, 'Test accuracy at {}, should be equal to or greater than 0.80'.format(test_accuracy)\n",
        "print('Nice Job! Test Accuracy is {}'.format(test_accuracy))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  1/4: 100%|██████████| 1425/1425 [00:02<00:00, 556.00batches/s]\n",
            "Epoch  2/4: 100%|██████████| 1425/1425 [00:02<00:00, 530.87batches/s]\n",
            "Epoch  3/4: 100%|██████████| 1425/1425 [00:02<00:00, 537.11batches/s]\n",
            "Epoch  4/4: 100%|██████████| 1425/1425 [00:02<00:00, 543.16batches/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-2445e5f56601>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Test accuracy at {}, should be equal to or greater than 0.80'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Nice Job! Test Accuracy is {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Test accuracy at 0.10000000149011612, should be equal to or greater than 0.80"
          ]
        }
      ]
    }
  ]
}